{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lesson 2: Prompt Engineering Fundamentals\n",
    "\n",
    "In this lesson, you'll learn how to craft effective prompts to get better results from LLMs.\n",
    "\n",
    "## Topics Covered\n",
    "1. System vs User messages\n",
    "2. Temperature and sampling parameters\n",
    "3. Few-shot learning\n",
    "4. Prompt templates and best practices\n",
    "5. Output formatting techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the role of system and user messages\n",
    "- Control creativity and determinism with parameters\n",
    "- Use examples to guide model behavior\n",
    "- Structure prompts for consistent outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "#%pip install python-dotenv\n",
    "#%pip install openai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "print(\"OpenAI package version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "client_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI client with environment variables\n",
    "chat_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    timeout=int(os.getenv(\"OPENAI_TIMEOUT\", 30)),\n",
    "    max_retries=int(os.getenv(\"MAX_RETRIES\", 3)),\n",
    "    base_url=os.getenv(\"OPENAI_ENDPOINT\")    \n",
    ")\n",
    "\n",
    "print(\"Client configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. System vs User Messages\n",
    "\n",
    "The **system message** sets the behavior and personality of the AI.\n",
    "The **user message** is the actual request or question.\n",
    "\n",
    "Think of it like:\n",
    "- System = \"You are a professional chef\"\n",
    "- User = \"How do I make pasta?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1A: Generic response (no system message)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain photosynthesis\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"WITHOUT System Message:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1B: Targeted response (with system message)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a 5th grade science teacher. Explain concepts simply using everyday examples.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain photosynthesis\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"WITH System Message (5th grade teacher):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Temperature and Sampling Parameters\n",
    "\n",
    "**Temperature** controls randomness/creativity:\n",
    "- `0.0` = Deterministic, focused, consistent\n",
    "- `1.0` = Creative, varied, unpredictable\n",
    "- `2.0` = Very creative (can be chaotic)\n",
    "\n",
    "**Top_p** (nucleus sampling): Alternative to temperature\n",
    "**Max_tokens**: Controls response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2A: Low temperature (deterministic)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a one-sentence story about a robot.\"}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Temperature 0.0 (Run this multiple times - notice consistency):\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2B: High temperature (creative)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a one-sentence story about a robot.\"}\n",
    "    ],\n",
    "    temperature=1.8\n",
    ")\n",
    "\n",
    "print(\"Temperature 1.8 (Run this multiple times - notice variety):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2C: Controlling length with max_tokens\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"List 10 programming languages\"}\n",
    "    ],\n",
    "    max_tokens=50  # Limit response length\n",
    ")\n",
    "\n",
    "print(\"With max_tokens=50:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens used: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Learning\n",
    "\n",
    "**Few-shot prompting** = Providing examples to guide the model's behavior.\n",
    "\n",
    "Instead of just telling the AI what to do, you show it examples of input â†’ output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3A: Zero-shot (no examples)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate to pirate speak: Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Zero-shot (no examples):\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3B: Few-shot (with examples)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You translate English to pirate speak.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Good morning\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Top o' the mornin' to ye!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where is the treasure?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Where be the booty?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Few-shot (with examples):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3C: Few-shot for structured output\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract product name and price from text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I bought a laptop for $899\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Product: laptop | Price: $899\"},\n",
    "        {\"role\": \"user\", \"content\": \"The coffee maker costs $49.99\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Product: coffee maker | Price: $49.99\"},\n",
    "        {\"role\": \"user\", \"content\": \"I just purchased wireless headphones for $129\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Few-shot for data extraction:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates and Best Practices\n",
    "\n",
    "Create reusable prompt templates for common tasks.\n",
    "\n",
    "### Best Practices:\n",
    "1. **Be specific** - Vague prompts = vague results\n",
    "2. **Use delimiters** - Separate instructions from content\n",
    "3. **Specify format** - Tell the model how to structure output\n",
    "4. **Give context** - Provide relevant background information\n",
    "5. **Use constraints** - Set length, style, or content boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4A: Bad prompt (vague)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about Python\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Vague Prompt:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4B: Good prompt (specific with constraints)\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a programming tutor for beginners.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Explain Python programming language.\n",
    "        \n",
    "Requirements:\n",
    "- Target audience: Complete beginners\n",
    "- Length: 3-4 sentences\n",
    "- Include: What it's used for and one key benefit\n",
    "- Tone: Encouraging and simple\"\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Specific Prompt with Constraints:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4C: Reusable template function\n",
    "def summarize_text(text, word_limit=50):\n",
    "    \"\"\"Reusable template for text summarization\"\"\"\n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional summarization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Summarize the following text in {word_limit} words or less:\n",
    "            \n",
    "Text to summarize:\n",
    "---\n",
    "{text}\n",
    "---\n",
    "\n",
    "Summary:\"\"\"}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the template\n",
    "long_text = \"\"\"Artificial intelligence is transforming the way we live and work. \n",
    "Machine learning algorithms can now recognize patterns in data, make predictions, \n",
    "and even generate creative content. Large language models like GPT can understand \n",
    "and generate human-like text, opening up new possibilities for automation, \n",
    "customer service, content creation, and education.\"\"\"\n",
    "\n",
    "summary = summarize_text(long_text, word_limit=30)\n",
    "print(\"Using Reusable Template:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Output Formatting Techniques\n",
    "\n",
    "Guide the model to produce structured outputs like JSON, bullet points, or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5A: JSON output\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You extract information and return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Extract information from this text and return as JSON:\n",
    "        \n",
    "Text: \"John Smith, 28 years old, works as a Software Engineer at TechCorp in Seattle.\"\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "  \"name\": \"...\",\n",
    "  \"age\": ...,\n",
    "  \"occupation\": \"...\",\n",
    "  \"company\": \"...\",\n",
    "  \"location\": \"...\"\n",
    "}\"\"\"}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"JSON Output:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Parse the JSON\n",
    "import json\n",
    "try:\n",
    "    data = json.loads(response.choices[0].message.content)\n",
    "    print(\"\\nParsed successfully! Name:\", data[\"name\"])\n",
    "except:\n",
    "    print(\"\\nCouldn't parse as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5B: Markdown table\n",
    "response = chat_client.chat.completions.create(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Create a comparison table of Python, JavaScript, and Java.\n",
    "        \n",
    "Format as markdown table with columns: Language, Type, Primary Use, Difficulty\n",
    "Keep it brief - one word or short phrase per cell.\"\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Markdown Table Output:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: System Message Design\n",
    "Create a system message that makes the AI act as a:\n",
    "- Tech support agent\n",
    "- Poet\n",
    "- Code reviewer\n",
    "\n",
    "### Exercise 2: Temperature Experiment\n",
    "Use the same prompt with temperatures 0, 0.7, and 1.5. Compare outputs.\n",
    "\n",
    "### Exercise 3: Few-Shot Email Classifier\n",
    "Create a few-shot prompt that classifies emails as: \"Urgent\", \"Normal\", or \"Spam\"\n",
    "\n",
    "### Exercise 4: Structured Data Extraction\n",
    "Extract movie information (title, year, genre) from text and output as JSON.\n",
    "\n",
    "### Exercise 5: Build a Template\n",
    "Create a reusable function that translates text to any language with consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "# Exercise 1:\n",
    "\n",
    "\n",
    "# Exercise 2:\n",
    "\n",
    "\n",
    "# Exercise 3:\n",
    "\n",
    "\n",
    "# Exercise 4:\n",
    "\n",
    "\n",
    "# Exercise 5:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **System messages** set AI behavior - use them to define role and style\n",
    "2. **Temperature** controls creativity: low (0-0.3) for consistency, high (0.7-2.0) for variety\n",
    "3. **Few-shot learning** shows examples of desired input/output patterns\n",
    "4. **Specific prompts** with constraints yield better results than vague requests\n",
    "5. **Structured outputs** (JSON, tables) make AI responses easier to process programmatically\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lesson 3, we'll explore:\n",
    "- Streaming responses for real-time output\n",
    "- Building interactive chat interfaces\n",
    "- Handling partial responses and errors\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Best Practices for Prompt Engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
